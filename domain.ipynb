{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9d2547",
   "metadata": {},
   "source": [
    "# Importing libraries and dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111ad0bc-4708-4be5-a090-02986a660303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cleaning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Category</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excited to watch the new superhero movie tonight!</td>\n",
       "      <td>cinema</td>\n",
       "      <td>excited watch new superhero movie tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest smartphone features are impressive</td>\n",
       "      <td>technology</td>\n",
       "      <td>latest smartphone features impressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our team won the championship! What a game</td>\n",
       "      <td>sports</td>\n",
       "      <td>team championship game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enjoyed the new action-packed movie last night</td>\n",
       "      <td>cinema</td>\n",
       "      <td>enjoyed new movie last night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can't believe our team made it to the finals</td>\n",
       "      <td>sports</td>\n",
       "      <td>ca believe team made finals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>The precision in archery and the strategy in c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>precision archery strategy chess share element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>The commitment and training required in sports...</td>\n",
       "      <td>sports</td>\n",
       "      <td>commitment training required sports mirror ded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>The dynamics of teamwork in sports teams echo ...</td>\n",
       "      <td>sports</td>\n",
       "      <td>dynamics teamwork sports teams echo importance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Cinema takes us on emotional journeys sparking...</td>\n",
       "      <td>cinema</td>\n",
       "      <td>cinema takes us emotional journeys sparking im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>The addictive nature of digital devices raises...</td>\n",
       "      <td>technology</td>\n",
       "      <td>addictive nature digital devices raises concer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1113 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets    Category  \\\n",
       "0     Excited to watch the new superhero movie tonight!      cinema   \n",
       "1         The latest smartphone features are impressive  technology   \n",
       "2            Our team won the championship! What a game      sports   \n",
       "3        Enjoyed the new action-packed movie last night      cinema   \n",
       "4          Can't believe our team made it to the finals      sports   \n",
       "...                                                 ...         ...   \n",
       "1130  The precision in archery and the strategy in c...      sports   \n",
       "1132  The commitment and training required in sports...      sports   \n",
       "1133  The dynamics of teamwork in sports teams echo ...      sports   \n",
       "1135  Cinema takes us on emotional journeys sparking...      cinema   \n",
       "1160  The addictive nature of digital devices raises...  technology   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0             excited watch new superhero movie tonight  \n",
       "1                 latest smartphone features impressive  \n",
       "2                                team championship game  \n",
       "3                          enjoyed new movie last night  \n",
       "4                           ca believe team made finals  \n",
       "...                                                 ...  \n",
       "1130  precision archery strategy chess share element...  \n",
       "1132  commitment training required sports mirror ded...  \n",
       "1133  dynamics teamwork sports teams echo importance...  \n",
       "1135  cinema takes us emotional journeys sparking im...  \n",
       "1160  addictive nature digital devices raises concer...  \n",
       "\n",
       "[1113 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Book2.csv')\n",
    "\n",
    "# Define a function for text cleaning\n",
    "def clean_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing URLs and User Mentions\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@[\\w_]+', '', text)\n",
    "    \n",
    "    # Tokenization and Removing Punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the 'Tweets' column\n",
    "data['cleaned_text'] = data['Tweets'].apply(clean_text)\n",
    "\n",
    "# Removing Duplicates\n",
    "data.drop_duplicates(subset=['cleaned_text'], inplace=True)\n",
    "\n",
    "# Handling Missing Values\n",
    "data.dropna(subset=['cleaned_text'], inplace=True)\n",
    "\n",
    "# Now, 'data' contains the cleaned text ready for further analysis\n",
    "# Optionally, you can save the cleaned data to a new CSV file\n",
    "data.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"done cleaning\")\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c15c06",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23c2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Load cleaned data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Assuming you have a 'Category' column for labels\n",
    "X = data['Tweets']  # Feature: Tweets\n",
    "y = data['Category']  # Labels: Category\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Now, X_tfidf contains the TF-IDF features\n",
    "# You can use X_tfidf for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3c228",
   "metadata": {},
   "source": [
    "# Data Splitting and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b602177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 967)\t0.3242062454023185\n",
      "  (0, 126)\t0.3130248707063763\n",
      "  (0, 507)\t0.29214694508464695\n",
      "  (0, 184)\t0.2887689400861184\n",
      "  (0, 141)\t0.2194351792618703\n",
      "  (0, 440)\t0.3307691182296963\n",
      "  (0, 655)\t0.29574303262248713\n",
      "  (0, 738)\t0.25900985453566816\n",
      "  (0, 461)\t0.3130248707063763\n",
      "  (0, 248)\t0.24394927258795795\n",
      "  (0, 574)\t0.2501467669410691\n",
      "  (0, 365)\t0.2099094077293855\n",
      "  (0, 603)\t0.08097613684050885\n",
      "  (0, 885)\t0.15853617812617907\n",
      "  (0, 41)\t0.07666163891249851\n",
      "  (0, 886)\t0.06548026421655631\n",
      "  (0, 902)\t0.100357564641463\n",
      "  (1, 137)\t0.4587754771154074\n",
      "  (1, 828)\t0.37779536996395574\n",
      "  (1, 492)\t0.22606896634821608\n",
      "  (1, 661)\t0.4076827180661518\n",
      "  (1, 32)\t0.34014176443363026\n",
      "  (1, 41)\t0.1014146622606449\n",
      "  (1, 774)\t0.4587754771154074\n",
      "  (1, 49)\t0.27459312857209306\n",
      "  :\t:\n",
      "  (221, 165)\t0.3447777595522013\n",
      "  (221, 60)\t0.17154184938636138\n",
      "  (221, 113)\t0.25017202361894814\n",
      "  (221, 529)\t0.18631170051913482\n",
      "  (221, 492)\t0.16505958660602368\n",
      "  (221, 41)\t0.07404582105598524\n",
      "  (221, 486)\t0.3131438092485012\n",
      "  (221, 613)\t0.14957689377439826\n",
      "  (221, 15)\t0.25815980446928133\n",
      "  (221, 886)\t0.1264919455325467\n",
      "  (221, 902)\t0.09693320386142594\n",
      "  (222, 697)\t0.37087355553783974\n",
      "  (222, 10)\t0.37087355553783974\n",
      "  (222, 37)\t0.2749702018852267\n",
      "  (222, 402)\t0.29080294974896553\n",
      "  (222, 249)\t0.2812484352461349\n",
      "  (222, 439)\t0.25933641281722886\n",
      "  (222, 51)\t0.33475507225730194\n",
      "  (222, 114)\t0.3021875690603911\n",
      "  (222, 60)\t0.1899310441140572\n",
      "  (222, 387)\t0.20566025050262876\n",
      "  (222, 994)\t0.2410589377526027\n",
      "  (222, 603)\t0.08659750414716659\n",
      "  (222, 41)\t0.16396698713176006\n",
      "  (222, 886)\t0.21007772190537977\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "domain_classifier = SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "domain_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(X_test)\n",
    "# Predict the labels on the testing data\n",
    "y_pred = domain_classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059c1f8",
   "metadata": {},
   "source": [
    "# Manual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130c4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket match never forget\n",
      "Predicted Label: sports\n",
      "Predicted Domain: sports\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Manual input for prediction\n",
    "\n",
    "manual_input = \"This is a cricket match that i will never forget\"\n",
    "\n",
    "\n",
    "cleaned_input = clean_text(manual_input)\n",
    "print(cleaned_input)\n",
    "# Vectorize the cleaned input using the TF-IDF vectorize\n",
    "\n",
    "\n",
    "vectorized_input = tfidf_vectorizer.transform([cleaned_input])\n",
    "# Predict the domain label on the manual input\n",
    "\n",
    "predicted_domain_label = domain_classifier.predict(vectorized_input)\n",
    "\n",
    "print(\"Predicted Label:\", predicted_domain_label[0])\n",
    "\n",
    "\n",
    "# Convert the predicted label to the corresponding domain category\n",
    "predicted_domain_category = predicted_domain_label[0]\n",
    "\n",
    "print(\"Predicted Domain:\", predicted_domain_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09882cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain_classifier_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(domain_classifier, 'domain_classifier_model.pkl')\n",
    "# Save the TF-IDF vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f9d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classification Metrics:\n",
      "Accuracy: 0.9596412556053812\n",
      "Precision: 0.9630261405342738\n",
      "Recall: 0.9596412556053812\n",
      "F1 Score: 0.9599076125104458\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      cinema       0.99      0.95      0.97        88\n",
      "      sports       0.90      1.00      0.95        73\n",
      "  technology       1.00      0.92      0.96        62\n",
      "\n",
      "    accuracy                           0.96       223\n",
      "   macro avg       0.96      0.96      0.96       223\n",
      "weighted avg       0.96      0.96      0.96       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "# Assuming you have already trained your classifier and made predictions as mentioned earlier\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision, recall, and F1-score (you can specify the average and labels as needed)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # You can change 'average' as needed\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # You can change 'average' as needed\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # You can change 'average' as needed\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the metrics and classification report\n",
    "print(\"Domain Classification Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
